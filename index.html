<!doctype html>
<html>
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

    <title>4Catalyzer Presentation</title>

    <link rel="stylesheet" href="css/reveal.css">
    <link rel="stylesheet" href="css/theme/white.css">

    <!-- Theme used for syntax highlighting of code -->
    <link rel="stylesheet" href="lib/css/tomorrow.css">

    <!-- Printing and PDF exports -->
    <script>
      var link = document.createElement( 'link' );
      link.rel = 'stylesheet';
      link.type = 'text/css';
      link.href = window.location.search.match( /print-pdf/gi ) ? 'css/print/pdf.css' : 'css/print/paper.css';
      document.getElementsByTagName( 'head' )[0].appendChild( link );
    </script>
  </head>
  <body>
    <div class="reveal">
      <div class="slides">
	<section>
	  <h2> scikit-image graphs &
	  Unsupervised learning with videos</h2>
	  <h4> - by Vighnesh Birodkar </h4>
	</section>
	<section>
	  <h2> About Me </h2>
	  <ul>
	    <li> 2nd year Master's student at NYU Courant studying Computer Science</li>
	    <li> Have always been interested in computer vision. </li>
	    <li> Open source contributor for <b> scikit-image </b> and <b> scikit-learn </b>. </li>
	    <li> Currently Research Assistant with Prof. Davi Geiger at Courant. </li>
	  </ul>
	  <p>
	  Blog : <a href="https://vcansimplify.wordpress.com/">
	      https://vcansimplify.wordpress.com/</a>
	  </p>
	  <p>
	  Website: <a href= "http://vighneshbirodkar.github.io/">
	      http://vighneshbirodkar.github.io/</a>
	  </p>
	</section>
	<section>
	  <img width="500px" src="images/minsky.jpg" />
	  <h6> Marvin Minsky, Co-founder of MIT's AI Lab </h6>
	  <p>
	    <i>
	      In the 60s, Marvin Minsky assigned a couple of undergrads to spend the summer programming a
	      computer to use a camera to identify objects in a scene. He figured they'd have the problem
	      solved by the end of the summer. Half a century later, we're still working on it.
	    </i>
	    [https://www.xkcd.com/1425/]
	  </p>
	</section>
	<section>
	  <section>
	    <h2> Region Adjacency Graphs (RAGs)</h2>
	    <h3> For scikit-image</h3>
	    <p>My Summer Project.</p>
	  </section>
	  <section>
	    <h2>Background</h2>
	    <ul>
	      <li>It's the summer of 2014. I had recently finished my graduation. </li>
	      <li>Recently accepted into the GSoC program for <b>scikit-image</b>.</li>
	      <li>Mentored by <u>St√©fan Van Der Walt</u> and <u>Juan Nunez-Iglesias</u> </li>
	    </ul>
	  </section>
	  <section>
	    <h2>Over-segmentation</h2>
	    <img width="800px" src="images/lena_oversegment.png" />
	  </section>
	  <section>
	    <h2>The Goal</h2>
	    <img width="800px" src="images/lena_rag.png" />
	    <p>Create a Region Adjacency Graph and leverage it for a better segmentation.</p>
	  </section>
	  <section>
	    <h2>Data structure choices</h2>
	    <ul>
	      <li> Should handle graphs in ND images. In a 3D segmented image you could have
		close to 100,000 nodes.</li>
	      <li> This ruled out dense representations like adjacency matrix. </li>
	      <li> We evaluated 4 choices, using Networkx(NX), List of Lists (LIL),
		Scipy's CSR matrix (CSR), and our own implementation (Custom) which
		used a list of dictionaries </li>
	      <li>To simulate a typical use case we built a RAG of 3 images and randomly
	      merged nodes till only 10 nodes are left.</li>
	    </ul>
	  </section>
	  <section>
	    <h2>Time Usage</h2>
	    <img width="800px" src="images/rag_time.png" />
	  </section>
	  <section>
	    <h2>Memory Usage</h2>
	    <img width="800px" src="images/rag_mem.png" />
	  </section>
	  <section>
	    <h2>Conclusion</h2>
	    <ul>
	      <li>Finalized using networkx library.</li>
	      <li>Would allow using their existing graph manipulation routines.</li>
	      <li>Pure python, hence no problem having it as a dependency.</li>
	      <li>Created the <b>RAG</b> class inside <b>skimage</b> which inherits
		from <b>networkx.Graph</b>
	    </ul>
	  </section>
	  <section>
	    <h2>Example - Input Image</h2>
	    <img width="800px" src="images/car.jpg" />
	  </section>
	  <section>
	    <h2>Example - Initial Segmentation</h2>
	    <img width="800px" src="images/car_slic.png" />
	  </section>
	  <section>
	    <h2>Code - Contruction</h2>
    <pre><code data-trim data-noescape class="python">
img = io.imread('car.jpg')
labels = segmentation.slic(img, compactness=30, n_segments=400)
g = graph.rag_mean_color(img, labels)

out = graph.draw_rag(labels, g, img)
    </code></pre>
	  </section>
	  <section>
	    <h2>Example - RAG</h2>
	    <img width="800px" src="images/car_rag.png" />
	  </section>
	  <section>
	    <h2> Example - Thresholding </h2>
    <pre><code data-trim data-noescape class="python">
labels_new = graph.cut_threshold(labels, g, 30)

    </code></pre>
    <img width="800px" src="images/car_thresh.png" />
	  </section>
	  <section>
	    <h2> Images and graph cut </h2>
	    <ul>
	      <li>If similar regions have high edge weights, it seems natural that
		a minimum cut remove low weight edges preserve regions.</li>
	      <li> However </li>
	    </ul>
	     <img width="500px" src="images/min_cut.jpg" />
	  </section>
	  <section>
	    <h2> Normalized Cut </h2>
	    For a graph V.
	    $$
	    \ \\
	    NCut(A, B) = \frac{cut(A, B)}{Assoc(A, V)} + \frac{cut(A, B)}{Assoc(B, V)}\\
	    cut(A, B) = \sum\limits_{a \in A, b \in B} w(a, b)\\
	    Assoc(X, V) = \sum\limits_{x \in X, v\in V } w(x, v) 
	    $$
	  </section>
	  <section>
	    <h2>Normalized Cut</h2>
	    <img width="500px" src="images/ncut.jpg" />
	  </section>
	  <section>
	    <h2> Example - NCut </h2>
<pre><code data-trim data-noescape class="python">
graph.rag_mean_color(img, labels, mode='similarity')		
labels_new = graph.cut_normalized(labels, g)
    </code></pre>
<img width="800px" src="images/car_ncut.png" />
	  </section>
	  <section>
	    <h2>NCut - Video</h2>
	    <video controls>
	      <source src="images/ncut_car.mp4" type="video/mp4">
	    </video>
	  </section>
	  <section>
	    <h2>NCut - Image</h2>
	    <img width="800px" src="images/fruit.jpg" />
	  </section>
	  <section>
	    <h2>NCut - Video</h2>
	    <video controls>
	      <source src="images/fruit.webm" type="video/webm">
	    </video>
	  </section>
	  <section>
	    <h2>Hierarchical Merging</h2>
	    <ul>
	      <li> 
	      Repeatedly group similar adjacent regions until there
	      are no adjacent regions which are similar enough.
	      </li>
	      <li>
		Implemented in scikit-image via 2 callbacks.
	      </li>
	    </ul>
	  </section>
	  <section>
	    <h2> Weight callback </h2>
<pre><code data-trim data-noescape class="python">
def _weight_mean_color(graph, src, dst, n):
    """Callback to handle merging nodes by recomputing mean color.

    The method expects that the mean color of `dst` is already computed.

    Parameters
    ----------
    graph : RAG
        The graph under consideration.
    src, dst : int
        The vertices in `graph` to be merged.
    n : int
        A neighbor of `src` or `dst` or both.

    Returns
    -------
    data : dict
        A dictionary with the `"weight"` attribute set as the absolute
        difference of the mean color between node `dst` and `n`.
    """

    diff = graph.node[dst]['mean color'] - graph.node[n]['mean color']
    diff = np.linalg.norm(diff)
    return {'weight': diff}
</code> </pre>
	  </section>
	  <section>
	    <h2>Merge callback</h2>
<pre><code data-trim data-noescape class="python">
def merge_mean_color(graph, src, dst):
    """Callback called before merging two nodes of a mean color distance graph.

    This method computes the mean color of `dst`.

    Parameters
    ----------
    graph : RAG
        The graph under consideration.
    src, dst : int
        The vertices in `graph` to be merged.
    """
    graph.node[dst]['total color'] += graph.node[src]['total color']
    graph.node[dst]['pixel count'] += graph.node[src]['pixel count']
    graph.node[dst]['mean color'] = (graph.node[dst]['total color'] /
</code> </pre>
	  </section>
	  <section>
	    <h3> Example - Hierarchical Merging </h3>
<pre><code data-trim data-noescape class="python">
labels_new = graph.merge_hierarchical(labels, g, thresh=35, rag_copy=False,
                                   in_place_merge=True, merge_func=merge_mean_color,
                                   weight_func=_weight_mean_color)
</code> </pre>
            <img width="800px" src="images/car_merge.png" />
	  </section>
	  <section>
	    <h2>Hierarchical Merging - Video</h2>
	    <video controls>
	      <source src="images/car_merge.webm" type="video/webm">
	    </video>
	  </section>
	  <section>
	    <h2>Region Boundary RAGs</h2>
	    <ul>
	      <li> Use an energy map from the user, typically an edge map </li>
	      <li> The edge weight between 2 regions is the average value of the
	      energy map along their shared boundary.</li>
	    </ul>
	  </section>
	  <section>
	    <h2>Example - Region Boundary RAG</h2>
<pre><code data-trim data-noescape class="python">
edges = filters.sobel(gray_img)
g = graph.rag_boundary(labels, edges)
</code> </pre>
            <img width="800px" src="images/rag_boundary.png" />
	  </section>
	  <section>
	    <h2>Region Boundary RAG callbacks</h2>
<pre><code data-trim data-noescape class="python">
def weight_boundary(graph, src, dst, n):
    """
    Handle merging of nodes of a region boundary region adjacency graph.

    This function computes the `"weight"` and the count `"count"`
    attributes of the edge between `n` and the node formed after
    merging `src` and `dst`.


    Parameters
    ----------
    graph : RAG
        The graph under consideration.
    src, dst : int
        The vertices in `graph` to be merged.
    n : int
        A neighbor of `src` or `dst` or both.

    Returns
    -------
    data : dict
        A dictionary with the "weight" and "count" attributes to be
        assigned for the merged node.

    """
    default = {'weight': 0.0, 'count': 0}

    count_src = graph[src].get(n, default)['count']
    count_dst = graph[dst].get(n, default)['count']

    weight_src = graph[src].get(n, default)['weight']
    weight_dst = graph[dst].get(n, default)['weight']

    count = count_src + count_dst
    return {
        'count': count,
        'weight': (count_src * weight_src + count_dst * weight_dst)/count
    }


def merge_boundary(graph, src, dst):
    """Call back called before merging 2 nodes.

    In this case we don't need to do any computation here.
    """
    pass
</code> </pre>
	  </section>
	  <section>
	    <h2>Example - Boundary Merge</h2>
<pre><code data-trim data-noescape class="python">
labels2 = graph.merge_hierarchical(labels, g, thresh=0.1, rag_copy=False,
                                   in_place_merge=True, merge_func=merge_boundary,
                                   weight_func=weight_boundary)
</code> </pre>
             <img width="800px" src="images/car_boundary_merge.png" />
	  </section>
	  <section>
	    <h2>Summary</h2>
	    <ul>
	      <li><b>skimage.future.graph</b> provides APIs to construct and manimulate
		n-dimensional region adjacency graphs.</li>
	      <li>Well documented and hackable.</li>
	      <li>Useful for pre-processing while training ConvNets.</li>
	      <li>More details about my summer project
		<a href="https://vcansimplify.wordpress.com/tag/gsoc/" > On my blog</a>
	      </li>
	    </ul>
	  </section>
	</section>
	
	<section>
	  <section>
	    <h2>Unsupervised learning in videos</h2>
	    <p>Research Assistantship during Fall 2016 with Prof. Davi Geiger.</p>
	    <p style="color:#ff0000" >[Work in progress]</p>
	  </section>
	  <section>
	    <h2>Our understanding of motion.</h2>
	    <ul>
	      <li>Humans understand motion really well.</li>
	      <li>We are able to anticipate motion, infer trajectories and react
		to motion very quickly.</li>
	      <li>Most of our learning about motion comes in an unsupervised setting.</li>
	      <li>We think that humans are able to map motion to a linear space to be able
	      to interpolate and extrapolate so well.</li>
	    </ul>
	  </section>
	  <section>
	    <h2>Compression</h2>
	    Knowledge about the domain is extremely essential for good compression.
	    
	    <p>
	    Rather than specifying 640 x 480 pixels, humans would describe the picture
	    as "Red Ford GT sports car with yellow stripes parked on a road with trees
	    in the background."</p>
	    <img width="400px" src="images/redcar.jpg" />
	  </section>
	  <section>
	    <h2>Autoencoders</h2>
	    Given input image $X$, Encoder $E$ and Decoder $D$, we have
	    $$
	    z = E(X)\\
	    D(z) \approx X
	    $$
	    Typically $size(z) << size(X)$. If the model is able to
	successfully achieve its objective, it would have gained some
	knowledge about the domain, specifically images.
	</section>
	  <section>
	    <h2>Our approach</h2>
	    <ul>
	      <li> Encode and decode frames trough time minimizing
		reconstruction error. </li>
	      <li>Force the latent vectors of frames to be linear through.</li>
	      <li>One way to encourage linearity is cosine distance.</li>
	    </ul>
	  </section>
	  <section>
	    <img style="border:none" width="800px" src="images/emb.png" />
	  </section>
	  <section>
	    <h2>Cost function</h2>
	    Given sequence of frames $X_1, X_2, \ldots X_n$
	    $$
	    \mathcal{L} = \sum\limits_{i=1}^N \lVert D(E(X_i)) - X_i \rVert_2^2
	    - \sum\limits_{i=2}^{n-1} d_{\mathrm{cosine}}\mid (X_i - X_1, X_n - X_1) \mid\\
	    d_{\mathrm{cosine}}(a, b) = \frac{a \cdot b}{\lVert a\rVert_2 \lVert b\rVert_2}
	    $$
	  </section>
	  <section>
	    <h1>Results so far</h1> 
	  </section>
	  <section>
	    <h6>No penalty</h6>
	    <img width="500px" src="images/demo/output_normal/output005.png" />
	    <h6>Cosine distance penalty</h6>
	    <img width="500px" src="images/demo/output_cosine/output005.png" />
	  </section>
	  <section>
	    <h6>No penalty</h6>
	    <img width="500px" src="images/demo/output_normal/output045.png" />
	    <h6>Cosine distance penalty</h6>
	    <img width="500px" src="images/demo/output_cosine/output045.png" />
	  </section>
	  <section>
	    <h6>No penalty</h6>
	    <img width="500px" src="images/demo/output_normal/output092.png" />
	    <h6>Cosine distance penalty</h6>
	    <img width="500px" src="images/demo/output_cosine/output092.png" />
	  </section>
	  <section>
	    <h6>No penalty</h6>
	    <img width="500px" src="images/demo/output_normal/output099.png" />
	    <h6>Cosine distance penalty</h6>
	    <img width="500px" src="images/demo/output_cosine/output099.png" />
	  </section>
	  <section>
	    <h2>Possible improvements</h2>
	    <ul>
	      <li>MSE leads to blurry predictions in multi-modal distributions.
	      An adversarial penalty could be used to address this.</li>
	      <li>The assumption fails when there is a change of scene in a video.
	      Possibly avoid by pre-processing.</li>
	    </ul>
	  </section>
	</section>
	<section>
	  <h2> Thank You </h2>
	</section>
      </div>
    </div>

    <script src="lib/js/head.min.js"></script>
    <script src="js/reveal.js"></script>

    <script>
      // More info https://github.com/hakimel/reveal.js#configuration
      Reveal.initialize({
      history: true,

      // More info https://github.com/hakimel/reveal.js#dependencies
      dependencies: [
      { src: 'plugin/math/math.js', async: true },
      { src: 'plugin/markdown/marked.js' },
      { src: 'plugin/markdown/markdown.js' },
      { src: 'plugin/notes/notes.js', async: true },
      { src: 'plugin/highlight/highlight.js', async: true, callback: function() { hljs.initHighlightingOnLoad(); } }
      ]
      });
    </script>
  </body>
</html>
